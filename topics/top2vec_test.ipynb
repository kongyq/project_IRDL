{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "\n",
    "model_file = \"/home/cds/Documents/Models/top2vec.model\"\n",
    "\n",
    "model = Top2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def topic_parser(file):\n",
    "    topics = dict()\n",
    "    with open(file, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            elems = line.split('|')\n",
    "            topics[int(elems[0])] = elems[1]\n",
    "    return topics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "topics_file = \"/home/cds/Documents/Topics/topics_full_2.txt\"\n",
    "\n",
    "topics = topic_parser(topics_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['government', 'assistance', 'airbus', 'industrie', 'trade', 'dispute', 'airbus', 'aircraft', 'producer', 'issue', 'subsidy']\n"
     ]
    }
   ],
   "source": [
    "keywords = topics[51].split(\" \")\n",
    "\n",
    "keywords.remove(\"document\")\n",
    "keywords.remove(\"mention\")\n",
    "keywords.remove(\"discuss\")\n",
    "print(keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "document_scores, document_ids = model.search_documents_by_keywords(keywords=keywords, num_docs=10000)\n",
    "#for score, doc_id in zip(document_scores, document_ids):\n",
    "#    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "  #  print(\"-----------\")\n",
    " #   print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def save_runs(doc_ids, doc_scores, topic_no, file):\n",
    "    with open(file,  'a') as fp:\n",
    "        for i, (doc_id, doc_score) in enumerate(zip(doc_ids, doc_scores)):\n",
    "            fp.write(str(topic_no) + \" Q0 \" + doc_id + \" \" + str(i) + \" \" + str(doc_score) + \" top2vec\" + \"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "run_file = \"/home/cds/Documents/Runs/run_desc.txt\"\n",
    "\n",
    "save_runs(document_ids, document_scores, 51, run_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "trcv_manifest = {\"trcv1\":[\"wsj87\", \"wsj88\", \"wsj89\", \"fr89\", \"ap89\", \"doe\", \"zf1\"],\n",
    "                 \"trcv2\":[\"wsj90\", \"wsj91\", \"wsj92\", \"fr88\", \"ap88\", \"zf2\"],\n",
    "                 \"trcv3\":[\"sjmn\", \"ap90\", \"pt\", \"zf3\"],\n",
    "                 \"trcv4\":[\"ft\", \"cr\", \"fr94\"],\n",
    "                 \"trcv5\":[\"fbis\", \"la\"],\n",
    "                 \"trcv4_nocr\":[\"ft\", \"fr94\"]}\n",
    "\n",
    "trec_manifest = {\"trec1\":[\"trcv1\", \"trcv2\"],\n",
    "                 \"trec2\":[\"trcv1\", \"trcv2\"],\n",
    "                 \"trec3\":[\"trcv1\", \"trcv2\"],\n",
    "                 \"trec4\":[\"trcv2\", \"trcv3\"],\n",
    "                 \"trec5\":[\"trcv2\", \"trcv4\"],\n",
    "                 \"trec6\":[\"trcv4\", \"trcv5\"],\n",
    "                 \"trec7\":[\"trcv4_nocr\", \"trcv5\"],\n",
    "                 \"trec8\":[\"trcv4_nocr\", \"trcv5\"]}\n",
    "\n",
    "def get_doc_prefix(trec_no):\n",
    "    prefix_list = set()\n",
    "    for trcv in trec_manifest[\"trec\"+str(trec_no)]:\n",
    "        for prefix in trcv_manifest[trcv]:\n",
    "            prefix_list.add(prefix)\n",
    "    return prefix_list\n",
    "\n",
    "def result_filter(doc_scores, doc_ids, trec_no):\n",
    "    filtered_scores = list()\n",
    "    filtered_ids = list()\n",
    "\n",
    "    prefixes = get_doc_prefix(trec_no)\n",
    "\n",
    "    for i, doc_id in enumerate(doc_ids):\n",
    "        for prefix in prefixes:\n",
    "            if doc_id.lower().startswith(prefix):\n",
    "                filtered_ids.append(doc_id)\n",
    "                filtered_scores.append(doc_scores[i])\n",
    "\n",
    "    return filtered_scores, filtered_ids\n",
    "\n",
    "def get_topic_list(trec_no):\n",
    "    return [i for i in range(50*trec_no+1, 50*(trec_no+1)+1)]\n",
    "\n",
    "def check_keywords(keywords):\n",
    "    oov_keywords = list()\n",
    "    filtered_keywords = list()\n",
    "    vocab = model.model.wv.vocab\n",
    "    for word in keywords:\n",
    "        if word not in vocab:\n",
    "            oov_keywords.append(word)\n",
    "        else:\n",
    "            filtered_keywords.append(word)\n",
    "    return filtered_keywords, oov_keywords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "trec_no = 5\n",
    "\n",
    "run_file = \"/home/cds/Documents/Runs/run_top2vec.txt\"\n",
    "\n",
    "oov = list()\n",
    "\n",
    "for topic_no in get_topic_list(trec_no):\n",
    "    keywords = topics[topic_no].split(\" \")\n",
    "    filtered_keywords, oov_keywords = check_keywords(keywords)\n",
    "    oov.extend(oov_keywords)\n",
    "    document_scores, document_ids = model.search_documents_by_keywords(keywords=filtered_keywords, num_docs=10000)\n",
    "    filtered_scores, filtered_ids = result_filter(document_scores, document_ids, trec_no)\n",
    "    save_runs(filtered_ids, filtered_scores, topic_no, run_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def search_documents_by_topkeys(model=model, keywords=keywords, num_docs=num_docs):\n",
    "    _,_,_,topics_nums = model.search_topics(keywords, 10)\n",
    "\n",
    "    for topics_no in topics_nums:\n",
    "        model.get_documents_by_topic()\n",
    "        model.search_documents_by_topic(topics_no, model.get, return_documents=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-b25fd1f42e27>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"apple\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sads\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"apple\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_word_vectors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test = [\"apple\", \"sads\"]\n",
    "t = [\"apple\"]\n",
    "model._get_word_vectors(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ds = model._get_document_vectors()\n",
    "vs = model.model.wv.vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def search_documents_by_tfidf(top2vec, keywords, topn=10000):\n",
    "    docvecs = top2vec._get_document_vectors()\n",
    "    keywords_valid = list()\n",
    "    for keyword in keywords:\n",
    "        if keyword not in model.model.wv.vocab:\n",
    "            print(keyword)\n",
    "        else:\n",
    "            keywords_valid.append(keyword)\n",
    "    keywords_vecs = top2vec._get_word_vectors(keywords_valid)\n",
    "\n",
    "    N = len(docvecs)\n",
    "    scores = np.zeros(N)\n",
    "    for word_vec in keywords_vecs:\n",
    "        sim_docs = model.model.docvecs.most_similar(positive=[word_vec], topn=None)\n",
    "\n",
    "        doc_tfs = np.array(sim_docs)\n",
    "\n",
    "        most_sim_top_nums, _ = model._search_vectors_by_vector(model.topic_vectors, word_vec, num_res=1)\n",
    "        nt = np.sum(model.doc_top == most_sim_top_nums[0])\n",
    "\n",
    "        idf = math.log(N / nt)\n",
    "\n",
    "        #print(nt, idf)\n",
    "        tfidf = np.multiply(doc_tfs, idf)\n",
    "        tfidf[tfidf < 0] = 0\n",
    "\n",
    "        for i in range(N):\n",
    "            scores[i] += tfidf[i]\n",
    "\n",
    "    indexes = np.flip(np.argsort(scores)[-topn:])\n",
    "    sorted_scores = np.array([round(scores[rank], 6) for rank in indexes])\n",
    "    doc_ids = model._get_document_ids(indexes)\n",
    "    return sorted_scores, doc_ids\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "ss, dd = search_documents_by_tfidf(model, [\"disorder\", \"syndrome\"], 1634243)\n",
    "print(np.min(ss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "wordvecs = model._get_word_vectors([\"disorder\"])\n",
    "res = model.model.docvecs.most_similar(positive=wordvecs, topn=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "minim = res.min()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "CPU times: user 4min 43s, sys: 1min 34s, total: 6min 17s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trec_no = 5\n",
    "\n",
    "run_file = \"/home/cds/Documents/Runs/run_top2vec_tfidf.txt\"\n",
    "\n",
    "oov = list()\n",
    "\n",
    "for topic_no in get_topic_list(trec_no):\n",
    "    print(topic_no)\n",
    "    keywords = topics[topic_no].split(\" \")\n",
    "    filtered_keywords, oov_keywords = check_keywords(keywords)\n",
    "    oov.extend(oov_keywords)\n",
    "\n",
    "    document_scores, document_ids = search_documents_by_tfidf(model, filtered_keywords, topn=10000)\n",
    "    filtered_scores, filtered_ids = result_filter(document_scores, document_ids, trec_no)\n",
    "    save_runs(filtered_ids, filtered_scores, topic_no, run_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def search_documents_by_euclidean(top2vec, keywords, topn=10000):\n",
    "    docvecs = top2vec._get_document_vectors()\n",
    "    keywords_valid = list()\n",
    "    for keyword in keywords:\n",
    "        if keyword not in model.model.wv.vocab:\n",
    "            print(keyword)\n",
    "        else:\n",
    "            keywords_valid.append(keyword)\n",
    "    keywords_vecs = top2vec._get_word_vectors(keywords_valid)\n",
    "\n",
    "    combined_vec = model._get_combined_vec(keywords_vecs, [])\n",
    "\n",
    "    results = euclidean_distances(docvecs, combined_vec.reshape(1,-1))\n",
    "    results = [r[0] for r in results]\n",
    "\n",
    "    scores = results\n",
    "    # scores = np.reciprocal(results)\n",
    "\n",
    "    indexes = np.flip(np.argsort(scores)[-topn:])\n",
    "    sorted_scores = np.array([scores[rank] for rank in indexes])\n",
    "    doc_ids = model._get_document_ids(indexes)\n",
    "    return sorted_scores, doc_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4039933073197139\n"
     ]
    }
   ],
   "source": [
    "ss, dd = search_documents_by_euclidean(model, [\"disorder\", \"syndrome\"], 100)\n",
    "print(np.min(ss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "CPU times: user 2min 31s, sys: 34.9 s, total: 3min 6s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trec_no = 5\n",
    "\n",
    "run_file = \"/home/cds/Documents/Runs/run_top2vec_euclidean_increase.txt\"\n",
    "\n",
    "oov = list()\n",
    "\n",
    "for topic_no in get_topic_list(trec_no):\n",
    "    print(topic_no)\n",
    "    keywords = topics[topic_no].split(\" \")\n",
    "    filtered_keywords, oov_keywords = check_keywords(keywords)\n",
    "    oov.extend(oov_keywords)\n",
    "\n",
    "    document_scores, document_ids = search_documents_by_euclidean(model, filtered_keywords, topn=10000)\n",
    "    filtered_scores, filtered_ids = result_filter(document_scores, document_ids, trec_no)\n",
    "    save_runs(filtered_ids, filtered_scores, topic_no, run_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def search_documents_by_queryexp(top2vec, keywords, topn=10000):\n",
    "\n",
    "    keywords_valid = list()\n",
    "    for keyword in keywords:\n",
    "        if keyword not in model.model.wv.vocab:\n",
    "            print(keyword)\n",
    "        else:\n",
    "            keywords_valid.append(keyword)\n",
    "    keywords_vecs = top2vec._get_word_vectors(keywords_valid)\n",
    "\n",
    "    candidate_vecs = list()\n",
    "    candidate_vecs.extend(keywords_vecs)\n",
    "    # N = len(docvecs)\n",
    "    # scores = np.zeros(N)\n",
    "    for word_vec in keywords_vecs:\n",
    "        # sim_docs = model.model.docvecs.most_similar(positive=[word_vec], topn=None)\n",
    "\n",
    "        # doc_tfs = np.array(sim_docs)\n",
    "\n",
    "        most_sim_top_nums, _ = model._search_vectors_by_vector(model.topic_vectors, word_vec, num_res=1)\n",
    "        most_sim_top_vec = model.topic_vectors[most_sim_top_nums]\n",
    "\n",
    "        # print(most_sim_top_vec[0])\n",
    "\n",
    "        candidate_vecs.append(most_sim_top_vec[0])\n",
    "\n",
    "    sim_docs = model.model.docvecs.most_similar(positive=candidate_vecs, topn=topn)\n",
    "    doc_indexes = [doc[0] for doc in sim_docs]\n",
    "    doc_scores = np.array([round(doc[1], 6) for doc in sim_docs])\n",
    "\n",
    "    return doc_scores, model._get_document_ids(doc_indexes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "CPU times: user 1min 7s, sys: 32.3 s, total: 1min 39s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trec_no = 5\n",
    "\n",
    "run_file = \"/home/cds/Documents/Runs/run_top2vec_queryexp_by_topicvecs.txt\"\n",
    "\n",
    "oov = list()\n",
    "\n",
    "for topic_no in get_topic_list(trec_no):\n",
    "    print(topic_no)\n",
    "    keywords = topics[topic_no].split(\" \")\n",
    "    filtered_keywords, oov_keywords = check_keywords(keywords)\n",
    "    oov.extend(oov_keywords)\n",
    "\n",
    "    document_scores, document_ids = search_documents_by_queryexp(model, filtered_keywords, topn=10000)\n",
    "    filtered_scores, filtered_ids = result_filter(document_scores, document_ids, trec_no)\n",
    "    save_runs(filtered_ids, filtered_scores, topic_no, run_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def search_documents_by_cluster(top2vec, keywords, topn=10000):\n",
    "\n",
    "    keywords_valid = list()\n",
    "    for keyword in keywords:\n",
    "        if keyword not in model.model.wv.vocab:\n",
    "            print(keyword)\n",
    "        else:\n",
    "            keywords_valid.append(keyword)\n",
    "    keywords_vecs = top2vec._get_word_vectors(keywords_valid)\n",
    "\n",
    "    combined_vec = model._get_combined_vec(keywords_vecs,[])\n",
    "\n",
    "    most_sim_top_nums,_ = model._search_vectors_by_vector(model.topic_vectors, combined_vec, num_res=50)\n",
    "\n",
    "    scores = model.model.docvecs.most_similar(positive=keywords_vecs, topn=None)\n",
    "\n",
    "    cand_docs_nums = list()\n",
    "\n",
    "    for topnum in most_sim_top_nums:\n",
    "        cand_docs_nums.extend(np.where(model.doc_top == topnum)[0])\n",
    "\n",
    "    cand_scores = [scores[i] for i in cand_docs_nums]\n",
    "\n",
    "    indexes = np.flip(np.argsort(cand_scores))\n",
    "    sorted_scores = np.array([round(cand_scores[rank], 6) for rank in indexes])\n",
    "\n",
    "    num_indexes = [cand_docs_nums[i] for i in indexes]\n",
    "\n",
    "    doc_ids = model._get_document_ids(num_indexes)\n",
    "    return sorted_scores, doc_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "CPU times: user 1min 6s, sys: 26.5 s, total: 1min 32s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trec_no = 5\n",
    "\n",
    "run_file = \"/home/cds/Documents/Runs/run_top2vec_clusters.txt\"\n",
    "\n",
    "oov = list()\n",
    "\n",
    "for topic_no in get_topic_list(trec_no):\n",
    "    print(topic_no)\n",
    "    keywords = topics[topic_no].split(\" \")\n",
    "    filtered_keywords, oov_keywords = check_keywords(keywords)\n",
    "    oov.extend(oov_keywords)\n",
    "\n",
    "    document_scores, document_ids = search_documents_by_cluster(model, filtered_keywords, topn=10000)\n",
    "    filtered_scores, filtered_ids = result_filter(document_scores, document_ids, trec_no)\n",
    "    save_runs(filtered_ids, filtered_scores, topic_no, run_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2, 5, 6, 7, 8]),)\n",
      "[1 2 3 4 5 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5, 3,3,3,3])\n",
    "b= np.where(a == 3)\n",
    "print(b)\n",
    "print(a)\n",
    "\n",
    "c = list()\n",
    "c.extend(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}